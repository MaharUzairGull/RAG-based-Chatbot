{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "790edd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class HybridUrduEmbeddings(Embeddings):\n",
    "    def __init__(self):\n",
    "        # Urdu-Optimized Model (Semantic)\n",
    "        self.urdu_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "        \n",
    "        # Numeric/Table-Optimized Model\n",
    "        self.num_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "           emb1 = self.urdu_model.embed_documents([text])[0]   # 768\n",
    "           emb2 = self.num_model.embed_documents([text])[0]    # 384\n",
    "           hybrid_emb = np.concatenate([emb1, emb2])           # 1152\n",
    "           embeddings.append(hybrid_emb.tolist())\n",
    "        return embeddings\n",
    "\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# Initialize\n",
    "hybrid_embeddings = HybridUrduEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9738f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index cotton-vectors with dimension 1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [06:42<00:00, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 774 document chunks to Pinecone index 'cotton-vectors'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API key and environment\n",
    "load_dotenv()\n",
    "# Make sure you have a .env file with:\n",
    "# PINECONE_API_KEY=your-key-here\n",
    "# PINECONE_ENV=your-env-here (e.g., \"gcp-starter\" or \"us-west4-gcp\")\n",
    "PINECONE_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Create Pinecone client instance (for Pinecone v3+)\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_KEY)\n",
    "\n",
    "# Create index (dimension must match embedding size)\n",
    "# Get the embedding dimension dynamically from the model\n",
    "test_emb = hybrid_embeddings.embed_documents([\"test\"])\n",
    "\n",
    "\n",
    "embedding_dim = 1152\n",
    "\n",
    "index_name = \"cotton-vectors\"\n",
    "if index_name not in pc.list_indexes():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    print(f\"Created index {index_name} with dimension {embedding_dim}\")\n",
    "else:\n",
    "    print(f\"Index {index_name} already exists\")\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Process the text file\n",
    "def process_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split by page markers\n",
    "    pages = content.split('===== Page ')[1:]\n",
    "    \n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for page in pages:\n",
    "        # Extract page info\n",
    "        try:\n",
    "            page_info, page_content = page.split(' =====', 1)\n",
    "            page_name = page_info.strip()\n",
    "            page_content = page_content.strip()\n",
    "            \n",
    "            # Split content into chunks\n",
    "            chunks = text_splitter.split_text(page_content)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Create a unique ID for each chunk\n",
    "                chunk_id = hashlib.md5(f\"{page_name}_{i}\".encode()).hexdigest()\n",
    "                \n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"page\": page_name,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"source\": \"extracted_text1.txt\"\n",
    "                })\n",
    "                \n",
    "        except ValueError:\n",
    "            print(f\"Skipping malformed page: {page[:100]}...\")\n",
    "    \n",
    "    return documents, metadatas\n",
    "\n",
    "# Path to your text file\n",
    "file_path = \"extracted_text1.txt\"\n",
    "\n",
    "# Process the file\n",
    "documents, metadatas = process_text_file(file_path)\n",
    "\n",
    "# Create embeddings and store in Pinecone\n",
    "batch_size = 32  # Adjust based on your resources\n",
    "\n",
    "# Use the Pinecone client instance (pc) to get the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    batch_metas = metadatas[i:i+batch_size]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = hybrid_embeddings.embed_documents(batch_docs)\n",
    "    \n",
    "    # Prepare upsert data\n",
    "    upsert_data = []\n",
    "    for doc, meta, embedding in zip(batch_docs, batch_metas, embeddings):\n",
    "        doc_id = meta.get(\"chunk_id\", hashlib.md5(doc.encode()).hexdigest())\n",
    "        # Pinecone v3 expects a dict with \"id\", \"values\", and \"metadata\"\n",
    "        upsert_data.append({\n",
    "            \"id\": doc_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\n",
    "                \"text\": doc,\n",
    "                **meta\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Upsert to Pinecone using the Pinecone v3 client\n",
    "    index.upsert(vectors=upsert_data)\n",
    "\n",
    "print(f\"Successfully uploaded {len(documents)} document chunks to Pinecone index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d9ae9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cotton-vectors\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(\"wheat-vectors\")\n",
    "print(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bae6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"fertilizer recommendations for weak land, specifying nutrient levels (organic matter, phosphorus, and potash) in kilograms per hectare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe519bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de9a5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain.schema import Document  # If your results are in LangChain Doc format\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment. Please set it in your .env file.\")\n",
    "\n",
    "# Step 1: Initialize Gemini with your API key\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Step 2: Load Gemini model (Gemini Pro for text tasks)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Step 3: Format vectorstore results\n",
    "def combine_documents(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "# Step 4: Generate response from Gemini\n",
    "def generate_response_from_docs(query: str, docs: list[Document]) -> str:\n",
    "    context = combine_documents(docs)\n",
    "    prompt = f\"\"\"You are a skillfull farmer. Answer the following question based only on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1d2d4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Gemini Answer:\n",
      " Based on the provided text, for weak land with organic matter â‰¤ 0.86%, phosphorus â‰¤ 7 ppm, and potash â‰¤ 80 ppm,  the fertilizer recommendation in kilograms per hectare is not explicitly given.  The text only states that a soil test should be conducted to determine the appropriate fertilizer application based on various factors including soil analysis results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Load the Pinecone vectorstore for the current index_name\n",
    "pinecone_vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=hybrid_embeddings,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "\n",
    "\n",
    "results = pinecone_vectorstore.similarity_search(query, k=3)\n",
    "final_answer = generate_response_from_docs(query, results)\n",
    "print(\"ðŸ¤– Gemini Answer:\\n\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74e415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
